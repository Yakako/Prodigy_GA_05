{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMXK72a7fmxzfljovTSuT1k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","from tensorflow.keras import layers, models\n","import cv2\n","from PIL import Image\n","import os\n","import requests # Import the requests library\n","\n","# For better performance, enable mixed precision if available\n","try:\n","    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n","    print(\"Mixed precision enabled\")\n","except:\n","    print(\"Using default precision\")\n","\n","class NeuralStyleTransfer:\n","    def __init__(self, content_path, style_path, img_size=512):\n","        \"\"\"\n","        Initialize Neural Style Transfer\n","\n","        Args:\n","            content_path: Path to content image\n","            style_path: Path to style image (artwork)\n","            img_size: Size to resize images to\n","        \"\"\"\n","        self.img_size = img_size\n","        self.content_image = self.load_and_preprocess_image(content_path)\n","        self.style_image = self.load_and_preprocess_image(style_path)\n","\n","        # Use VGG19 for feature extraction\n","        self.model = self.get_vgg19_model()\n","\n","        # Style layers and their weights\n","        self.style_layer_names = [\n","            'block1_conv1',  # Low-level features (edges, textures)\n","            'block2_conv1',  # Mid-level patterns\n","            'block3_conv1',  # Higher-level patterns\n","            'block4_conv1',  # Object parts\n","            'block5_conv1'   # Object-level features\n","        ]\n","\n","        # Weights for style layers\n","        self.style_layer_weights = [0.5, 1.0, 1.5, 3.0, 4.0]\n","\n","        # Content layer (deeper layer captures content)\n","        self.content_layer_name = 'block4_conv2'\n","\n","        # Weights for loss components\n","        self.content_weight = 1e-3\n","        self.style_weight = 1e-1\n","        self.tv_weight = 1e-4  # Total variation weight for smoothness\n","\n","    def load_and_preprocess_image(self, path):\n","        \"\"\"Load and preprocess image for VGG19\"\"\"\n","        img = tf.io.read_file(path)\n","        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n","        img = tf.image.convert_image_dtype(img, tf.float32)\n","\n","        # Resize while maintaining aspect ratio\n","        shape = tf.shape(img)[:-1]\n","        scale = tf.cast(self.img_size, tf.float32) / tf.cast(tf.maximum(shape[0], shape[1]), tf.float32)\n","        new_shape = tf.cast(tf.cast(shape, tf.float32) * scale, tf.int32)\n","        img = tf.image.resize(img, new_shape)\n","\n","        # Center crop to square\n","        img = tf.image.resize_with_crop_or_pad(img, self.img_size, self.img_size)\n","\n","        # Add batch dimension\n","        img = tf.expand_dims(img, 0)\n","\n","        # Convert to VGG19 format (mean subtraction)\n","        img = tf.keras.applications.vgg19.preprocess_input(img * 255)\n","\n","        return img\n","\n","    def deprocess_image(self, img):\n","        \"\"\"Convert preprocessed image back to displayable format\"\"\"\n","        img = img.numpy().squeeze()\n","\n","        # Reverse VGG19 preprocessing\n","        img[:, :, 0] += 103.939\n","        img[:, :, 1] += 116.779\n","        img[:, :, 2] += 123.68\n","\n","        # BGR to RGB and clip\n","        img = img[:, :, ::-1]\n","        img = np.clip(img, 0, 255).astype('uint8')\n","\n","        return img\n","\n","    def get_vgg19_model(self):\n","        \"\"\"Create VGG19 model with intermediate layer outputs\"\"\"\n","        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","        vgg.trainable = False\n","\n","        # Get outputs for style and content layers\n","        style_outputs = [vgg.get_layer(name).output for name in self.style_layer_names]\n","        content_outputs = [vgg.get_layer(self.content_layer_name).output]\n","\n","        # Combine all outputs\n","        outputs = style_outputs + content_outputs\n","\n","        model = tf.keras.Model(vgg.input, outputs)\n","\n","        return model\n","\n","    def gram_matrix(self, feature_map):\n","        \"\"\"Compute Gram matrix for style representation\"\"\"\n","        # Reshape feature map: (height, width, channels) -> (height * width, channels)\n","        shape = tf.shape(feature_map)\n","        channels = shape[-1]\n","        a = tf.reshape(feature_map, [-1, channels])\n","\n","        # Compute Gram matrix\n","        gram = tf.matmul(a, a, transpose_a=True)\n","\n","        # Normalize by number of locations\n","        gram = gram / tf.cast(tf.reduce_prod(shape[:-1]), tf.float32)\n","\n","        return gram\n","\n","    def compute_loss(self, generated_image, content_features, style_features):\n","        \"\"\"\n","        Compute total loss: content loss + style loss + total variation loss\n","        \"\"\"\n","        # Get features for generated image\n","        generated_features = self.model(generated_image)\n","\n","        # Split into style and content features\n","        gen_style_features = generated_features[:len(self.style_layer_names)]\n","        gen_content_features = generated_features[len(self.style_layer_names):]\n","\n","        # Content loss (L2 distance between content features)\n","        content_loss = 0\n","        for gen_feat, content_feat in zip(gen_content_features, content_features):\n","            content_loss += tf.reduce_mean(tf.square(gen_feat - content_feat))\n","\n","        # Style loss (L2 distance between Gram matrices)\n","        style_loss = 0\n","        for gen_feat, style_feat, weight in zip(gen_style_features, style_features, self.style_layer_weights):\n","            # Compute Gram matrices\n","            gen_gram = self.gram_matrix(gen_feat)\n","            style_gram = self.gram_matrix(style_feat)\n","\n","            # Compute loss for this layer\n","            layer_loss = tf.reduce_mean(tf.square(gen_gram - style_gram))\n","            style_loss += weight * layer_loss\n","\n","        # Total variation loss for smoothness\n","        tv_loss = tf.image.total_variation(generated_image)\n","\n","        # Combine losses\n","        total_loss = (self.content_weight * content_loss +\n","                     self.style_weight * style_loss +\n","                     self.tv_weight * tv_loss)\n","\n","        return total_loss, content_loss, style_loss, tv_loss\n","\n","    def transfer_style(self, num_iterations=1000, learning_rate=0.02, save_progress=True):\n","        \"\"\"\n","        Perform neural style transfer\n","\n","        Args:\n","            num_iterations: Number of optimization iterations\n","            learning_rate: Learning rate for optimization\n","            save_progress: Whether to save intermediate results\n","\n","        Returns:\n","            Final stylized image\n","        \"\"\"\n","        print(\"Starting neural style transfer...\")\n","\n","        # Precompute content and style features\n","        content_features = self.model(self.content_image)\n","        content_features = content_features[len(self.style_layer_names):]\n","\n","        style_features = self.model(self.style_image)\n","        style_features = style_features[:len(self.style_layer_names)]\n","\n","        # Initialize generated image with content image\n","        generated_image = tf.Variable(self.content_image, dtype=tf.float32)\n","\n","        # Optimizer\n","        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n","\n","        # Training loop\n","        for i in range(num_iterations):\n","            with tf.GradientTape() as tape:\n","                tape.watch(generated_image)\n","                total_loss, c_loss, s_loss, tv = self.compute_loss(\n","                    generated_image, content_features, style_features\n","                )\n","\n","            # Compute gradients and update\n","            gradients = tape.gradient(total_loss, generated_image)\n","            optimizer.apply_gradients([(gradients, generated_image)])\n","\n","            # Clip values to valid range\n","            generated_image.assign(tf.clip_by_value(generated_image, 0, 255))\n","\n","            # Print progress\n","            if i % 100 == 0:\n","                print(f\"Iteration {i}: Total Loss = {total_loss:.4f}, \"\n","                      f\"Content = {c_loss:.4f}, Style = {s_loss:.4f}, TV = {tv:.4f}\")\n","\n","                # Save intermediate result\n","                if save_progress:\n","                    result = self.deprocess_image(generated_image)\n","                    plt.figure(figsize=(8, 8))\n","                    plt.imshow(result)\n","                    plt.title(f'Iteration {i}')\n","                    plt.axis('off')\n","                    plt.savefig(f'progress_iter_{i}.png', bbox_inches='tight', pad_inches=0)\n","                    plt.close()\n","\n","        # Final result\n","        final_image = self.deprocess_image(generated_image)\n","\n","        return final_image\n","\n","    def display_results(self, final_image):\n","        \"\"\"Display content, style, and final images\"\"\"\n","        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","        # Content image\n","        content_display = self.deprocess_image(self.content_image)\n","        axes[0].imshow(content_display)\n","        axes[0].set_title('Content Image')\n","        axes[0].axis('off')\n","\n","        # Style image\n","        style_display = self.deprocess_image(self.style_image)\n","        axes[1].imshow(style_display)\n","        axes[1].set_title('Style Image (Artwork)')\n","        axes[1].axis('off')\n","\n","        # Final image\n","        axes[2].imshow(final_image)\n","        axes[2].set_title('Stylized Image')\n","        axes[2].axis('off')\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Save final image\n","        Image.fromarray(final_image).save('stylized_result.png')\n","        print(\"Final image saved as 'stylized_result.png'\")\n","\n","# Example usage\n","def download_sample_images():\n","    \"\"\"Download sample images if they don't exist\"\"\"\n","    # import urllib.request # No longer needed, using requests\n","\n","    # Sample images (you can replace these with your own)\n","    images = {\n","        'content.jpg': 'https://raw.githubusercontent.com/pytorch/examples/main/fast_neural_style/images/content-images/neural_style_dancing.jpg',\n","        'style.jpg': 'https://raw.githubusercontent.com/pytorch/examples/main/fast_neural_style/images/style-images/neural_style_picasso.jpg'\n","    }\n","\n","    # Define a User-Agent header to mimic a web browser\n","    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n","\n","    for filename, url in images.items():\n","        if not os.path.exists(filename):\n","            print(f\"Downloading {filename}...\")\n","            try:\n","                response = requests.get(url, headers=headers)\n","                response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n","                with open(filename, 'wb') as f:\n","                    f.write(response.content)\n","                print(f\"Successfully downloaded {filename}\")\n","            except requests.exceptions.RequestException as e:\n","                print(f\"Error downloading {filename}: {e}\")\n","                print(\"Please check the URL or provide local image files.\")\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Download sample images\n","    download_sample_images()\n","\n","    # Create style transfer object\n","    stylizer = NeuralStyleTransfer(\n","        content_path='content.jpg',\n","        style_path='style.jpg',\n","        img_size=512\n","    )\n","\n","    # Perform style transfer\n","    final_image = stylizer.transfer_style(\n","        num_iterations=1000,\n","        learning_rate=0.02,\n","        save_progress=True\n","    )\n","\n","    # Display results\n","    stylizer.display_results(final_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":499},"id":"cMty5I-Qg8Cf","executionInfo":{"status":"error","timestamp":1771162723211,"user_tz":-420,"elapsed":231,"user":{"displayName":"PRUONH KIMLIYA","userId":"05110817587043319602"}},"outputId":"3e86fe3c-6cb3-4f9e-fb68-56a3db770af1"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Mixed precision enabled\n","Downloading content.jpg...\n","Error downloading content.jpg: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/pytorch/examples/main/fast_neural_style/images/content-images/neural_style_dancing.jpg\n","Please check the URL or provide local image files.\n","Downloading style.jpg...\n","Error downloading style.jpg: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/pytorch/examples/main/fast_neural_style/images/style-images/neural_style_picasso.jpg\n","Please check the URL or provide local image files.\n"]},{"output_type":"error","ename":"NotFoundError","evalue":"{{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} content.jpg; No such file or directory [Op:ReadFile]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3800131292.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;31m# Create style transfer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     stylizer = NeuralStyleTransfer(\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mcontent_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mstyle_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'style.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3800131292.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, content_path, style_path, img_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \"\"\"\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3800131292.py\u001b[0m in \u001b[0;36mload_and_preprocess_image\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m\"\"\"Load and preprocess image for VGG19\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_animations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m\"string\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m   \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    581\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       return read_file_eager_fallback(\n\u001b[0m\u001b[1;32m    584\u001b[0m           filename, name=name, ctx=_ctx)\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    604\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001b[0m\u001b[1;32m    607\u001b[0m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[1;32m    608\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} content.jpg; No such file or directory [Op:ReadFile]"]}]}]}